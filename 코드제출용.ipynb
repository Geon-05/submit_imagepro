{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전략\n",
    "모델을 세가지로 나누어서 한단계한단계 복원을 진행하도록한다.  \n",
    "\n",
    "## 모델\n",
    "1. 마스크생성모델  \n",
    "  \n",
    "학습:  \n",
    "train_gt의 정답이미지를 흑백이미지로 변환하여 train_input의 학습이미지와 비교하여 차이가나는부분을 mask로하고 이것을 새로운 정답이미지로 사용한다.  \n",
    "이후 train_input파일과 mask이미지를 가지고 손상영역 mask 모델을 학습한다.  \n",
    "  \n",
    "테스트:  \n",
    "손상된부분을 인식하여 mask를 생성하고 각각의 파일을 mask폴더에 저장한다.  \n",
    "  \n",
    "2. 컬러복원모델  \n",
    "  \n",
    "학습:  \n",
    "train_gt의 이미지를 mask와 결합하여 손상을 시켜 이것을 새로운 정답이미지로 사용한다.  \n",
    "train_input파일과 손상된 이미지를 가지고 컬러복원 모델을 학습한다.  \n",
    "  \n",
    "테스트:  \n",
    "test_input파일에서 mask를 적용하여 mask를 제외한 나머지부분의 color를 복원하여 output_grayTocol폴더에 저장한다.\n",
    "  \n",
    "3. 손상복원모델  \n",
    "\n",
    "학습:  \n",
    "(2)에서 의도하여 손상시킨 손상된 컬러 이미지와 train_gt파일을 가지고 손상복원 모델을 학습한다.  \n",
    "  \n",
    "테스트:  \n",
    "컬러만 복원된 파일에서 mask로 손상된부분을 인식시키고 그 손상된부분을 복원하여 최종 폴더에 저장한다.\n",
    "단, 컬러복원 모델의 컬러복원성능이 좀더 좋기때문에 이전에 컬러복원된부분은 그대로 사용하도록한다.\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "## 전처리 부가함수\n",
    "1. 마스크생성기  \n",
    "  \n",
    "손상된부분의 마스크를 생성한다.\n",
    "\n",
    "2. 컬러손상이미지생성기\n",
    "\n",
    "손상된 컬러이미지를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 마스크생성모델  \n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch = 30\n",
    "data_ratio = 1 # 100% 데이터셋사용\n",
    "num_epochs = 60\n",
    "test_size=0.2\n",
    "lr=0.001\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset and DataLoader\n",
    "input_dir = '/root/.cache/kagglehub/datasets/geon05/dataset2/versions/1/train_input'\n",
    "gt_dir = '/root/.cache/kagglehub/datasets/geon05/dataset2/versions/1/train_gt'\n",
    "\n",
    "# Train-Validation Split (80:20)\n",
    "image_files = sorted(os.listdir(input_dir))\n",
    "mask_files = sorted(os.listdir(gt_dir))\n",
    "\n",
    "# 10% 샘플링된 데이터에서 Train-Validation Split (80:20)\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    image_files, mask_files, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "class DamageDataset(Dataset):\n",
    "    def __init__(self, input_dir, gt_dir, image_files, mask_files, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.image_files[idx])\n",
    "        gt_path = os.path.join(self.gt_dir, self.mask_files[idx])\n",
    "\n",
    "        # Load and preprocess images\n",
    "        input_image = Image.open(input_path).convert(\"RGB\")\n",
    "        input_image_np = np.array(input_image)\n",
    "        gt_image_gray = Image.open(gt_path).convert(\"L\")\n",
    "        gt_image_gray_np = np.array(gt_image_gray)\n",
    "\n",
    "        input_image_gray_np = cv2.cvtColor(input_image_np, cv2.COLOR_RGB2GRAY)\n",
    "        difference = cv2.absdiff(gt_image_gray_np, input_image_gray_np)\n",
    "        _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        binary_difference = cv2.morphologyEx(binary_difference, cv2.MORPH_CLOSE, kernel)\n",
    "        contours, _ = cv2.findContours(binary_difference, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        mask_filled = np.zeros_like(binary_difference)\n",
    "        cv2.drawContours(mask_filled, contours, -1, color=255, thickness=cv2.FILLED)\n",
    "        mask_filled = cv2.dilate(mask_filled, kernel, iterations=1)\n",
    "\n",
    "        input_tensor = transforms.ToTensor()(input_image)\n",
    "        mask_tensor = torch.tensor(mask_filled, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "        return input_tensor, mask_tensor\n",
    "\n",
    "# Training and Validation Datasets\n",
    "train_dataset = DamageDataset(input_dir, gt_dir, train_images, train_masks)\n",
    "val_dataset = DamageDataset(input_dir, gt_dir, val_images, val_masks)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "# Load model with updated weights parameter\n",
    "weights = DeepLabV3_ResNet50_Weights.DEFAULT\n",
    "model = deeplabv3_resnet50(weights=weights)\n",
    "model.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 주 GPU로 cuda:0을 사용하도록 설정\n",
    "\n",
    "# DataParallel 및 SyncBatchNorm 적용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs! Model training started...\")\n",
    "    model = nn.DataParallel(model)  # DataParallel로 모델 병렬화\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)  # SyncBatchNorm 변환\n",
    "\n",
    "# GPU로 모델 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training and Validation Loop\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            outputs = model(inputs)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model, f\"best_model_{epoch+1}_{best_val_loss:.4f}.pth\")\n",
    "        print(f\"  Best model saved with Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Visualization (Optional)\n",
    "    with torch.no_grad():\n",
    "        inputs, masks = next(iter(val_loader))\n",
    "        inputs, masks = inputs[:5].to(device), masks[:5].to(device)\n",
    "        predictions = torch.sigmoid(model(inputs)['out'])\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        masks = masks.cpu().numpy()\n",
    "        inputs = inputs.cpu().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(5, 3, figsize=(12, 15))\n",
    "        for i in range(5):\n",
    "            axes[i, 0].imshow(inputs[i].transpose(1, 2, 0))\n",
    "            axes[i, 0].set_title(\"Input Image\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "            axes[i, 1].imshow(masks[i][0], cmap=\"gray\")\n",
    "            axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "            axes[i, 2].imshow(predictions[i][0], cmap=\"gray\")\n",
    "            axes[i, 2].set_title(\"Predicted Mask\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagepro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
