{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전략  \n",
    "모델을 세 가지 단계로 나누어 각 단계에서 복원을 순차적으로 진행합니다.  \n",
    "  \n",
    "### 모델 구성  \n",
    "  \n",
    "1. 마스크 생성 모델  \n",
    "  \n",
    "학습 과정  \n",
    "  \n",
    "train_gt의 정답 이미지를 흑백 이미지로 변환합니다.  \n",
    "변환된 흑백 이미지와 train_input 학습 이미지를 비교하여 차이가 나는 부분을 마스크로 지정합니다.  \n",
    "생성된 마스크를 새로운 정답 이미지로 사용하여 학습합니다.  \n",
    "최종적으로 train_input 파일과 생성된 마스크 이미지를 활용해 손상 영역을 탐지하는 마스크 생성 모델을 학습합니다.  \n",
    "  \n",
    "테스트 과정  \n",
    "  \n",
    "손상된 부분을 인식하여 마스크를 생성합니다.  \n",
    "생성된 마스크를 mask 폴더에 저장합니다.  \n",
    "  \n",
    "2. 컬러 복원 모델  \n",
    "  \n",
    "학습 과정  \n",
    "  \n",
    "train_gt 이미지를 마스크와 결합하여 손상 이미지를 생성합니다.  \n",
    "이 손상 이미지를 새로운 정답 이미지로 사용하여 학습합니다.  \n",
    "train_input 파일과 손상 이미지를 이용해 컬러 복원 모델을 학습합니다.  \n",
    "  \n",
    "테스트 과정  \n",
    "  \n",
    "test_input 파일에 마스크를 적용하여 손상된 부분을 제외한 나머지 영역의 색상을 복원합니다.  \n",
    "복원된 이미지를 output_grayTocol 폴더에 저장합니다.  \n",
    "  \n",
    "3. 손상 복원 모델  \n",
    "  \n",
    "학습 과정  \n",
    "  \n",
    "(2)단계에서 의도적으로 손상시킨 컬러 이미지와 train_gt 파일을 활용하여 손상 복원 모델을 학습합니다.  \n",
    "  \n",
    "테스트 과정  \n",
    "  \n",
    "컬러 복원된 파일에서 마스크를 사용해 손상된 부분을 인식하고 복원합니다.  \n",
    "복원된 이미지를 최종 폴더에 저장합니다.  \n",
    "단, 컬러 복원 모델의 복원 성능이 우수한 부분은 그대로 사용합니다.  \n",
    "\n",
    "--------------------\n",
    "\n",
    "전처리 부가 함수  \n",
    "1. 마스크 생성기  \n",
    "손상된 부분의 마스크를 생성합니다.  \n",
    "  \n",
    "2. 컬러 손상 이미지 생성기  \n",
    "손상된 컬러 이미지를 생성합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 마스크생성모델  \n",
    "  \n",
    "![epoch14](https://github.com/Geon-05/submit_imagepro/blob/main/%EC%A0%9C%EC%B6%9C%EC%9A%A9%EC%9D%B4%EB%AF%B8%EC%A7%80/mask_model_epoch14%20copy.png?raw=true)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch = 30\n",
    "data_ratio = 1 # 100% 데이터셋사용\n",
    "num_epochs = 60\n",
    "test_size=0.2\n",
    "lr=0.001\n",
    "\n",
    "# Dataset and DataLoader\n",
    "input_dir = '/root/.cache/kagglehub/datasets/geon05/dataset2/versions/1/train_input'\n",
    "gt_dir = '/root/.cache/kagglehub/datasets/geon05/dataset2/versions/1/train_gt'\n",
    "\n",
    "# Train-Validation Split (80:20)\n",
    "image_files = sorted(os.listdir(input_dir))\n",
    "mask_files = sorted(os.listdir(gt_dir))\n",
    "\n",
    "# 10% 샘플링된 데이터에서 Train-Validation Split (80:20)\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    image_files, mask_files, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "class DamageDataset(Dataset):\n",
    "    def __init__(self, input_dir, gt_dir, image_files, mask_files, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.image_files[idx])\n",
    "        gt_path = os.path.join(self.gt_dir, self.mask_files[idx])\n",
    "\n",
    "        # Load and preprocess images\n",
    "        input_image = Image.open(input_path).convert(\"RGB\")\n",
    "        input_image_np = np.array(input_image)\n",
    "        gt_image_gray = Image.open(gt_path).convert(\"L\")\n",
    "        gt_image_gray_np = np.array(gt_image_gray)\n",
    "\n",
    "        input_image_gray_np = cv2.cvtColor(input_image_np, cv2.COLOR_RGB2GRAY)\n",
    "        difference = cv2.absdiff(gt_image_gray_np, input_image_gray_np)\n",
    "        _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        binary_difference = cv2.morphologyEx(binary_difference, cv2.MORPH_CLOSE, kernel)\n",
    "        contours, _ = cv2.findContours(binary_difference, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        mask_filled = np.zeros_like(binary_difference)\n",
    "        cv2.drawContours(mask_filled, contours, -1, color=255, thickness=cv2.FILLED)\n",
    "        mask_filled = cv2.dilate(mask_filled, kernel, iterations=1)\n",
    "\n",
    "        input_tensor = transforms.ToTensor()(input_image)\n",
    "        mask_tensor = torch.tensor(mask_filled, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "        return input_tensor, mask_tensor\n",
    "\n",
    "# Training and Validation Datasets\n",
    "train_dataset = DamageDataset(input_dir, gt_dir, train_images, train_masks)\n",
    "val_dataset = DamageDataset(input_dir, gt_dir, val_images, val_masks)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "\n",
    "# Load model with updated weights parameter\n",
    "weights = DeepLabV3_ResNet50_Weights.DEFAULT\n",
    "model = deeplabv3_resnet50(weights=weights)\n",
    "model.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 주 GPU로 cuda:0을 사용하도록 설정\n",
    "\n",
    "# DataParallel 및 SyncBatchNorm 적용\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs! Model training started...\")\n",
    "    model = nn.DataParallel(model)  # DataParallel로 모델 병렬화\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)  # SyncBatchNorm 변환\n",
    "\n",
    "# GPU로 모델 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training and Validation Loop\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            outputs = model(inputs)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model, f\"best_model_{epoch+1}_{best_val_loss:.4f}.pth\")\n",
    "        print(f\"  Best model saved with Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Visualization (Optional)\n",
    "    with torch.no_grad():\n",
    "        inputs, masks = next(iter(val_loader))\n",
    "        inputs, masks = inputs[:5].to(device), masks[:5].to(device)\n",
    "        predictions = torch.sigmoid(model(inputs)['out'])\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        masks = masks.cpu().numpy()\n",
    "        inputs = inputs.cpu().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(5, 3, figsize=(12, 15))\n",
    "        for i in range(5):\n",
    "            axes[i, 0].imshow(inputs[i].transpose(1, 2, 0))\n",
    "            axes[i, 0].set_title(\"Input Image\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "            axes[i, 1].imshow(masks[i][0], cmap=\"gray\")\n",
    "            axes[i, 1].set_title(\"Ground Truth Mask\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "            axes[i, 2].imshow(predictions[i][0], cmap=\"gray\")\n",
    "            axes[i, 2].set_title(\"Predicted Mask\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 저장된 모델 경로\n",
    "model_path = \"./best_model_30_0.0016.pth\"  # 저장된 모델 파일 경로\n",
    "\n",
    "# 모델 로드\n",
    "model = torch.load(model_path)\n",
    "\n",
    "# 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 경로\n",
    "test_input_dir = \"/path/to/test_input_images\"  # 테스트용 흑백 이미지 폴더\n",
    "\n",
    "# 테스트 데이터셋 정의\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_dir, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.image_files = sorted(os.listdir(input_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.image_files[idx])\n",
    "        input_image = Image.open(input_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "        return input_image, self.image_files[idx]  # 이미지와 파일명을 반환\n",
    "\n",
    "# Transform 설정\n",
    "test_transform = transforms.ToTensor()\n",
    "\n",
    "# 테스트 데이터셋 및 DataLoader 생성\n",
    "test_dataset = TestDataset(test_input_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 테스트 수행 및 시각화\n",
    "def test_and_visualize(model, test_loader, device, output_dir):\n",
    "    model.to(device)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, filenames in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)['out']\n",
    "            predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "            for i, pred in enumerate(predictions):\n",
    "                pred_mask = pred[0]  # (1, H, W) -> (H, W)\n",
    "                input_image = inputs[i].cpu().numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "                # 시각화 및 저장\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "                axes[0].imshow(input_image, cmap=\"gray\")\n",
    "                axes[0].set_title(\"Input Image\")\n",
    "                axes[0].axis(\"off\")\n",
    "\n",
    "                axes[1].imshow(pred_mask, cmap=\"gray\")\n",
    "                axes[1].set_title(\"Predicted Mask\")\n",
    "                axes[1].axis(\"off\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # 예측 결과 저장\n",
    "                save_path = os.path.join(output_dir, filenames[i])\n",
    "                pred_mask_uint8 = (pred_mask * 255).astype(np.uint8)\n",
    "                Image.fromarray(pred_mask_uint8).save(save_path)\n",
    "\n",
    "# 테스트 결과 저장 경로\n",
    "output_dir = \"./test_predictions\"\n",
    "\n",
    "# 테스트 및 시각화 수행\n",
    "test_and_visualize(model, test_loader, device, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 컬러복원모델\n",
    "\n",
    "![epoch14](https://github.com/Geon-05/submit_imagepro/blob/main/%EC%A0%9C%EC%B6%9C%EC%9A%A9%EC%9D%B4%EB%AF%B8%EC%A7%80/%EC%BB%AC%EB%9F%AC%EB%B3%B5%EC%9B%90.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "####################\n",
    "# 파라미터 설정\n",
    "####################\n",
    "gray_dir = \"/root/.cache/kagglehub/datasets/geon05/dataset2/versions/1/train_input\"\n",
    "color_dir = \"/root/.cache/kagglehub/datasets/geon05/damages-masks/versions/1/damage_images/damage_images\"\n",
    "mask_dir = \"/root/.cache/kagglehub/datasets/geon05/damages-masks/versions/1/output_masks/output_masks\"\n",
    "\n",
    "batch_size =20\n",
    "lr = 1e-4\n",
    "epochs = 50\n",
    "test_size = 0.2\n",
    "lambda_perc = 0.1  # Perceptual Loss 가중치\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "####################\n",
    "# Lab 변환 함수\n",
    "####################\n",
    "def rgb_to_lab_normalized(rgb):\n",
    "    rgb_np = (rgb.permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "    bgr = cv2.cvtColor(rgb_np, cv2.COLOR_RGB2BGR)\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2Lab).astype(np.float32)\n",
    "    L = lab[:,:,0] / 255.0\n",
    "    a = (lab[:,:,1] - 128.0)/128.0\n",
    "    b = (lab[:,:,2] - 128.0)/128.0\n",
    "    return L, a, b\n",
    "\n",
    "def lab_to_rgb(L, a, b):\n",
    "    lab_0_255 = np.zeros((L.shape[0], L.shape[1], 3), dtype=np.float32)\n",
    "    lab_0_255[:,:,0] = L * 255.0\n",
    "    lab_0_255[:,:,1] = a * 128.0 + 128.0\n",
    "    lab_0_255[:,:,2] = b * 128.0 + 128.0\n",
    "    lab_0_255 = np.clip(lab_0_255, 0, 255).astype(np.uint8)\n",
    "    bgr = cv2.cvtColor(lab_0_255, cv2.COLOR_Lab2BGR)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb = np.clip(rgb,0,255).astype(np.uint8)\n",
    "    return rgb / 255.0\n",
    "\n",
    "####################\n",
    "# SSIM, Histogram Similarity\n",
    "####################\n",
    "def ssim_score(true, pred):\n",
    "    return ssim(true, pred, channel_axis=-1, data_range=1.0)\n",
    "\n",
    "def histogram_similarity(true, pred):\n",
    "    true_bgr = cv2.cvtColor((true*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    pred_bgr = cv2.cvtColor((pred*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    true_hsv = cv2.cvtColor(true_bgr, cv2.COLOR_BGR2HSV)\n",
    "    pred_hsv = cv2.cvtColor(pred_bgr, cv2.COLOR_BGR2HSV)\n",
    "    hist_true = cv2.calcHist([true_hsv], [0], None, [180], [0, 180])\n",
    "    hist_pred = cv2.calcHist([pred_hsv], [0], None, [180], [0, 180])\n",
    "    hist_true = cv2.normalize(hist_true, hist_true).flatten()\n",
    "    hist_pred = cv2.normalize(hist_pred, hist_pred).flatten()\n",
    "    return cv2.compareHist(hist_true, hist_pred, cv2.HISTCMP_CORREL)\n",
    "\n",
    "####################\n",
    "# VGGPerceptualLoss 정의\n",
    "####################\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer_ids=[3,8]):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features\n",
    "        self.layers = nn.ModuleList([vgg[i] for i in range(max(layer_ids)+1)])\n",
    "        self.layer_ids = layer_ids\n",
    "        for param in self.layers.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        mean = torch.tensor([0.485,0.456,0.406], device=x.device).view(1,3,1,1)\n",
    "        std = torch.tensor([0.229,0.224,0.225], device=x.device).view(1,3,1,1)\n",
    "        x = (x - mean) / std\n",
    "        feats = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i in self.layer_ids:\n",
    "                feats.append(x)\n",
    "        return feats\n",
    "\n",
    "####################\n",
    "# UpConv 및 DoubleConv 클래스\n",
    "####################\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "####################\n",
    "# Dataset 정의 (마스크 추가)\n",
    "####################\n",
    "class DamagedGrayColorDataset(Dataset):\n",
    "    def __init__(self, gray_paths, color_paths, mask_paths, transform_gray=None, transform_color=None):\n",
    "        self.gray_paths = gray_paths\n",
    "        self.color_paths = color_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform_gray = transform_gray\n",
    "        self.transform_color = transform_color\n",
    "        assert len(self.gray_paths) == len(self.color_paths) == len(self.mask_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gray_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g_path = self.gray_paths[idx]\n",
    "        c_path = self.color_paths[idx]\n",
    "        m_path = self.mask_paths[idx]\n",
    "\n",
    "        gray_img = Image.open(g_path).convert('L')\n",
    "        color_img = Image.open(c_path).convert('RGB')\n",
    "        mask_img = Image.open(m_path).convert('L')  # 0~255 범위, 255=손상?\n",
    "\n",
    "        # 마스크 이진화\n",
    "        mask_np = np.array(mask_img)\n",
    "        mask_bin = (mask_np > 128).astype(np.float32)\n",
    "        mask_bin = torch.from_numpy(mask_bin).unsqueeze(0) # [1,H,W]\n",
    "\n",
    "        if self.transform_gray:\n",
    "            gray_img = self.transform_gray(gray_img)\n",
    "        if self.transform_color:\n",
    "            color_img = self.transform_color(color_img)\n",
    "\n",
    "        # 타겟 Lab 변환\n",
    "        L_t, a_t, b_t = rgb_to_lab_normalized(color_img)\n",
    "        a_t = torch.from_numpy(a_t).unsqueeze(0)\n",
    "        b_t = torch.from_numpy(b_t).unsqueeze(0)\n",
    "        ab_t = torch.cat([a_t, b_t], dim=0)\n",
    "        L_t = torch.from_numpy(L_t).unsqueeze(0)\n",
    "\n",
    "        # Gray->RGB->Lab L 변환\n",
    "        gray_3ch = torch.cat([gray_img,gray_img,gray_img], dim=0)\n",
    "        G_L, G_a, G_b = rgb_to_lab_normalized(gray_3ch)\n",
    "        G_L = torch.from_numpy(G_L).unsqueeze(0)\n",
    "\n",
    "        return G_L, ab_t, L_t, mask_bin\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_color = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "gray_files = sorted(glob.glob(os.path.join(gray_dir, \"*\")))\n",
    "color_files = sorted(glob.glob(os.path.join(color_dir, \"*\")))\n",
    "\n",
    "mask_files = []\n",
    "for gf in gray_files:\n",
    "    fname = os.path.basename(gf)\n",
    "    m_path = os.path.join(mask_dir, fname)\n",
    "    mask_files.append(m_path)\n",
    "\n",
    "train_gray, val_gray, train_color, val_color, train_mask, val_mask = train_test_split(\n",
    "    gray_files, color_files, mask_files, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = DamagedGrayColorDataset(train_gray, train_color, train_mask, transform_gray, transform_color)\n",
    "val_dataset = DamagedGrayColorDataset(val_gray, val_color, val_mask, transform_gray, transform_color)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "####################\n",
    "# ResNetEncoder (ResNet-50)\n",
    "####################\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        net = models.resnet50(weights=ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "        self.initial = nn.Sequential(net.conv1, net.bn1, net.relu)\n",
    "        self.maxpool = net.maxpool\n",
    "        self.layer1 = net.layer1 # 256채널\n",
    "        self.layer2 = net.layer2 # 512채널\n",
    "        self.layer3 = net.layer3 # 1024채널\n",
    "        self.layer4 = net.layer4 # 2048채널\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.initial(x)   #64채널\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)   #256채널\n",
    "        x2 = self.layer2(x1)   #512채널\n",
    "        x3 = self.layer3(x2)   #1024채널\n",
    "        x4 = self.layer4(x3)   #2048채널\n",
    "        return x0, x1, x2, x3, x4\n",
    "\n",
    "####################\n",
    "# 디코더 부분 (ResNet-50에 맞게)\n",
    "####################\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, out_ch=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(pretrained=pretrained)\n",
    "        self.up3 = UpConv(2048, 1024)\n",
    "        self.dec3 = DoubleConv(2048, 1024)\n",
    "        self.up2 = UpConv(1024, 512)\n",
    "        self.dec2 = DoubleConv(1024, 512)\n",
    "        self.up1 = UpConv(512, 256)\n",
    "        self.dec1 = DoubleConv(512, 256)\n",
    "        self.up0 = UpConv(256, 64)\n",
    "        self.dec0 = DoubleConv(128, 64)\n",
    "        self.up_final = UpConv(64,64)\n",
    "        self.dec_final = DoubleConv(64,64)\n",
    "        self.final_out = nn.Conv2d(64, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1,3,1,1)\n",
    "        x0, x1, x2, x3, x4 = self.encoder(x)\n",
    "        x_up3 = self.up3(x4)\n",
    "        x_cat3 = torch.cat([x_up3, x3], dim=1)\n",
    "        x_dec3 = self.dec3(x_cat3)\n",
    "\n",
    "        x_up2 = self.up2(x_dec3)\n",
    "        x_cat2 = torch.cat([x_up2, x2], dim=1)\n",
    "        x_dec2 = self.dec2(x_cat2)\n",
    "\n",
    "        x_up1 = self.up1(x_dec2)\n",
    "        x_cat1 = torch.cat([x_up1, x1], dim=1)\n",
    "        x_dec1 = self.dec1(x_cat1)\n",
    "\n",
    "        x_up0 = self.up0(x_dec1)\n",
    "        x_cat0 = torch.cat([x_up0, x0], dim=1)\n",
    "        x_dec0 = self.dec0(x_cat0)\n",
    "\n",
    "        x_upf = self.up_final(x_dec0)\n",
    "        x_decf = self.dec_final(x_upf)\n",
    "        out = self.final_out(x_decf)\n",
    "        return out\n",
    "\n",
    "model = ResNetUNet(out_ch=2, pretrained=True).to(device)\n",
    "perceptual_extractor = VGGPerceptualLoss(layer_ids=[3,8]).to(device)\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "def compute_loss(L_img, ab_img, pred_ab, L_t_img, mask):\n",
    "    # 마스크 제외한 영역만 손실 계산\n",
    "    diff = mse_loss(pred_ab, ab_img)  # MSE\n",
    "    valid_area = (1 - mask)\n",
    "    diff = diff * valid_area\n",
    "    denom = valid_area.sum() + 1e-8\n",
    "    l1_val = diff.sum() / denom\n",
    "\n",
    "    B = L_img.size(0)\n",
    "    ab_np = ab_img.permute(0,2,3,1).cpu().numpy()\n",
    "    pred_ab_np = pred_ab.permute(0,2,3,1).detach().cpu().numpy()\n",
    "    L_t_np = L_t_img[:,0].cpu().numpy()\n",
    "\n",
    "    true_rgb_list = []\n",
    "    pred_rgb_list = []\n",
    "    for i in range(B):\n",
    "        trgb = lab_to_rgb(L_t_np[i], ab_np[i][:,:,0], ab_np[i][:,:,1])\n",
    "        prgb = lab_to_rgb(L_t_np[i], pred_ab_np[i][:,:,0], pred_ab_np[i][:,:,1])\n",
    "        true_rgb_list.append(trgb)\n",
    "        pred_rgb_list.append(prgb)\n",
    "\n",
    "    true_rgb_t = torch.from_numpy(np.stack(true_rgb_list,axis=0)).float().to(device)\n",
    "    pred_rgb_t = torch.from_numpy(np.stack(pred_rgb_list,axis=0)).float().to(device)\n",
    "    true_rgb_t = true_rgb_t.permute(0,3,1,2)\n",
    "    pred_rgb_t = pred_rgb_t.permute(0,3,1,2)\n",
    "\n",
    "    true_feats = perceptual_extractor(true_rgb_t)\n",
    "    pred_feats = perceptual_extractor(pred_rgb_t)\n",
    "\n",
    "    perc_loss_val = torch.tensor(0.0, device=device)\n",
    "    for ft, fp in zip(true_feats, pred_feats):\n",
    "        perc_diff = (ft - fp)**2\n",
    "        down_mask = F.interpolate(valid_area, size=ft.shape[2:], mode='bilinear', align_corners=False)\n",
    "        down_mask_3ch = down_mask.repeat(1,ft.shape[1],1,1)\n",
    "        perc_diff = perc_diff * down_mask_3ch\n",
    "        perc_sum = perc_diff.sum()\n",
    "        perc_count = down_mask_3ch.sum() + 1e-8\n",
    "        perc_loss_val += perc_sum / perc_count\n",
    "\n",
    "    total_loss = l1_val + lambda_perc * perc_loss_val\n",
    "    return total_loss, l1_val, perc_loss_val\n",
    "\n",
    "def visualize_samples(model, data_loader, device, num_samples=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_iter = iter(data_loader)\n",
    "        L_batch, ab_batch, L_t_batch, mask_batch = next(val_iter)\n",
    "        L_batch = L_batch.to(device)\n",
    "        pred_ab = model(L_batch)\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        L_np = L_batch[i,0].cpu().numpy()\n",
    "        ab_true = ab_batch[i].permute(1,2,0).numpy()\n",
    "        L_t_np = L_t_batch[i,0].numpy()\n",
    "        pred_ab_np = pred_ab[i].cpu().permute(1,2,0).numpy()\n",
    "\n",
    "        true_rgb = lab_to_rgb(L_t_np, ab_true[:,:,0], ab_true[:,:,1])\n",
    "        pred_rgb = lab_to_rgb(L_t_np, pred_ab_np[:,:,0], pred_ab_np[:,:,1])\n",
    "\n",
    "        axes[i][0].imshow(L_np, cmap='gray')\n",
    "        axes[i][0].set_title(\"Damaged Gray Input\")\n",
    "        axes[i][0].axis('off')\n",
    "\n",
    "        axes[i][1].imshow(true_rgb)\n",
    "        axes[i][1].set_title(\"Target Color (using L_t)\")\n",
    "        axes[i][1].axis('off')\n",
    "\n",
    "        axes[i][2].imshow(pred_rgb)\n",
    "        axes[i][2].set_title(\"Predicted Color\")\n",
    "        axes[i][2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "checkpoint_path = \"/content/20241207_05_best_model_1_0.046.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"Loaded model state from {checkpoint_path}, continuing training...\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_combined_metric = float('-inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{epochs}] Training\")\n",
    "    for L_img, ab_img, L_t_img, mask in train_pbar:\n",
    "        L_img, ab_img, L_t_img, mask = L_img.to(device), ab_img.to(device), L_t_img.to(device), mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_ab = model(L_img)\n",
    "        total_loss, l1_val, perc_val = compute_loss(L_img, ab_img, pred_ab, L_t_img, mask)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "        train_pbar.set_postfix(loss=total_loss.item(), L1=l1_val.item(), Perc=perc_val.item())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    ssim_list = []\n",
    "    hist_sim_list = []\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{epochs}] Validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (L_img, ab_img, L_t_img, mask) in enumerate(val_pbar):\n",
    "            L_img, ab_img, L_t_img, mask = L_img.to(device), ab_img.to(device), L_t_img.to(device), mask.to(device)\n",
    "            pred_ab = model(L_img)\n",
    "            total_loss, l1_val, perc_val = compute_loss(L_img, ab_img, pred_ab, L_t_img, mask)\n",
    "            val_loss += total_loss.item()\n",
    "            val_pbar.set_postfix(val_loss=total_loss.item(), L1=l1_val.item(), Perc=perc_val.item())\n",
    "\n",
    "            # 마스크를 제외한 부분만을 반영한 SSIM/히스토그램 계산\n",
    "            # 첫 배치에 대해 몇 장만 측정 (원래 코드 로직 유지)\n",
    "            if i == 0:\n",
    "                num_samples = min(3, ab_img.size(0))\n",
    "                L_np_all = L_img[:,0].cpu().numpy()\n",
    "                ab_t_np_all = ab_img.permute(0,2,3,1).cpu().numpy()\n",
    "                pred_ab_np_all = pred_ab.permute(0,2,3,1).cpu().numpy()\n",
    "                L_t_np_all = L_t_img[:,0].cpu().numpy()\n",
    "                mask_np_all = mask[:,0].cpu().numpy()\n",
    "                for j in range(num_samples):\n",
    "                    true_rgb = lab_to_rgb(L_t_np_all[j], ab_t_np_all[j][:,:,0], ab_t_np_all[j][:,:,1])\n",
    "                    pred_rgb = lab_to_rgb(L_t_np_all[j], pred_ab_np_all[j][:,:,0], pred_ab_np_all[j][:,:,1])\n",
    "                    # 마스크 제외: masked area -> 0으로 set\n",
    "                    valid_area = (1 - mask_np_all[j]) # [H,W]\n",
    "                    valid_area_3ch = np.stack([valid_area]*3, axis=-1) # [H,W,3]\n",
    "\n",
    "                    # 마스크 영역 0으로 만들기\n",
    "                    true_rgb = true_rgb * valid_area_3ch\n",
    "                    pred_rgb = pred_rgb * valid_area_3ch\n",
    "\n",
    "                    current_ssim = ssim_score(true_rgb, pred_rgb)\n",
    "                    ssim_list.append(current_ssim)\n",
    "                    current_hist_sim = histogram_similarity(true_rgb, pred_rgb)\n",
    "                    hist_sim_list.append(current_hist_sim)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if len(ssim_list) > 0:\n",
    "        mean_ssim = np.mean(ssim_list)\n",
    "        mean_hist_sim = np.mean(hist_sim_list)\n",
    "        print(f\"Epoch [{epoch+1}] Mean SSIM (no-mask): {mean_ssim:.4f}, Mean Histogram Similarity (no-mask): {mean_hist_sim:.4f}\")\n",
    "        combined_metric = mean_ssim * mean_hist_sim\n",
    "        print(f\"Epoch [{epoch+1}] Combined Metric (no-mask): {combined_metric:.4f}\")\n",
    "\n",
    "        # 최적 모델은 마스크 제외 영역 품질 기준\n",
    "        if combined_metric > best_combined_metric:\n",
    "            best_combined_metric = combined_metric\n",
    "            torch.save(model.state_dict(), f\"20241207_05_best_model_{epoch+1}_{avg_val_loss:.3f}.pth\")\n",
    "            print(\"** Best Model Updated and Saved! **\")\n",
    "\n",
    "    visualize_samples(model, val_loader, device, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "#------------- 모델 정의 부분 (학습 시 사용한 코드와 동일해야 함) -------------#\n",
    "def rgb_to_lab_normalized(rgb):\n",
    "    rgb_np = (rgb.permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "    bgr = cv2.cvtColor(rgb_np, cv2.COLOR_RGB2BGR)\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2Lab).astype(np.float32)\n",
    "    L = lab[:,:,0] / 255.0\n",
    "    a = (lab[:,:,1] - 128.0)/128.0\n",
    "    b = (lab[:,:,2] - 128.0)/128.0\n",
    "    return L, a, b\n",
    "\n",
    "def lab_to_rgb(L, a, b):\n",
    "    lab_0_255 = np.zeros((L.shape[0], L.shape[1], 3), dtype=np.float32)\n",
    "    lab_0_255[:,:,0] = L * 255.0\n",
    "    lab_0_255[:,:,1] = a * 128.0 + 128.0\n",
    "    lab_0_255[:,:,2] = b * 128.0 + 128.0\n",
    "\n",
    "    lab_0_255 = np.clip(lab_0_255, 0, 255).astype(np.uint8)\n",
    "    bgr = cv2.cvtColor(lab_0_255, cv2.COLOR_Lab2BGR)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb = np.clip(rgb,0,255).astype(np.uint8)\n",
    "    return rgb / 255.0\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        net = models.resnet50(weights=ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "        self.initial = nn.Sequential(net.conv1, net.bn1, net.relu)\n",
    "        self.maxpool = net.maxpool\n",
    "        self.layer1 = net.layer1 # 256채널\n",
    "        self.layer2 = net.layer2 # 512채널\n",
    "        self.layer3 = net.layer3 # 1024채널\n",
    "        self.layer4 = net.layer4 # 2048채널\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.initial(x)   #64채널\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)   #256\n",
    "        x2 = self.layer2(x1)   #512\n",
    "        x3 = self.layer3(x2)   #1024\n",
    "        x4 = self.layer4(x3)   #2048\n",
    "        return x0, x1, x2, x3, x4\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, out_ch=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(pretrained=pretrained)\n",
    "        self.up3 = UpConv(2048, 1024)\n",
    "        self.dec3 = DoubleConv(2048, 1024)\n",
    "\n",
    "        self.up2 = UpConv(1024, 512)\n",
    "        self.dec2 = DoubleConv(1024, 512)\n",
    "\n",
    "        self.up1 = UpConv(512, 256)\n",
    "        self.dec1 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up0 = UpConv(256, 64)\n",
    "        self.dec0 = DoubleConv(128, 64)\n",
    "\n",
    "        self.up_final = UpConv(64,64)\n",
    "        self.dec_final = DoubleConv(64,64)\n",
    "        self.final_out = nn.Conv2d(64, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1,3,1,1)\n",
    "        x0, x1, x2, x3, x4 = self.encoder(x)\n",
    "\n",
    "        x_up3 = self.up3(x4)           \n",
    "        x_cat3 = torch.cat([x_up3, x3], dim=1) \n",
    "        x_dec3 = self.dec3(x_cat3)     \n",
    "\n",
    "        x_up2 = self.up2(x_dec3)       \n",
    "        x_cat2 = torch.cat([x_up2, x2], dim=1)\n",
    "        x_dec2 = self.dec2(x_cat2)     \n",
    "\n",
    "        x_up1 = self.up1(x_dec2)       \n",
    "        x_cat1 = torch.cat([x_up1, x1], dim=1)\n",
    "        x_dec1 = self.dec1(x_cat1)     \n",
    "\n",
    "        x_up0 = self.up0(x_dec1)       \n",
    "        x_cat0 = torch.cat([x_up0, x0], dim=1)\n",
    "        x_dec0 = self.dec0(x_cat0)\n",
    "\n",
    "        x_upf = self.up_final(x_dec0)\n",
    "        x_decf = self.dec_final(x_upf)\n",
    "        out = self.final_out(x_decf)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 로드\n",
    "model = ResNetUNet(out_ch=2, pretrained=False).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/zqrc05/project/imagepro/test/model/grayTocol/12_05_best_model_7_0.043.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 테스트 디렉토리 지정\n",
    "test_gray_dir = \"/home/zqrc05/project/imagepro/test/test_input\"  # 손상된 흑백 이미지들이 있는 폴더 경로\n",
    "test_mask_dir = \"/home/zqrc05/project/imagepro/test/mask\"  # 마스크 이미지들이 있는 폴더 경로\n",
    "output_dir = \"/home/zqrc05/project/imagepro/test/output_grayTocol\"     # 복원 결과를 저장할 폴더 경로\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# test_gray_dir 내 모든 png 이미지 처리\n",
    "gray_image_paths = sorted(glob.glob(os.path.join(test_gray_dir, \"*.png\")))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for gray_path in gray_image_paths:\n",
    "        fname = os.path.basename(gray_path)  # 예: TEST_001.png\n",
    "        mask_path = os.path.join(test_mask_dir, fname) # 동일 이름의 mask\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"No matching mask found for {fname}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 흑백 이미지 로드\n",
    "        gray_img = Image.open(gray_path).convert('L')\n",
    "        gray_tensor = transform_gray(gray_img) # [1,H,W]\n",
    "\n",
    "        # 마스크 로드\n",
    "        mask_img = Image.open(mask_path).convert('L')\n",
    "        mask_np = np.array(mask_img)\n",
    "        mask_bin = (mask_np > 128).astype(np.float32)\n",
    "        mask_bin = torch.from_numpy(mask_bin).unsqueeze(0) # [1,H,W]\n",
    "        # 사이즈가 다르다면 아래 주석 제거 후 interpolate 적용\n",
    "        # mask_bin = F.interpolate(mask_bin.unsqueeze(0), size=(512,512), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        gray_tensor = gray_tensor.unsqueeze(0).to(device) # [1,1,H,W]\n",
    "        mask_bin = mask_bin.to(device) # [1,H,W]\n",
    "\n",
    "        # 모델 추론\n",
    "        pred_ab = model(gray_tensor) # [1,2,H,W]\n",
    "\n",
    "        # 결과 복원\n",
    "        pred_ab_np = pred_ab[0].cpu().permute(1,2,0).numpy()  # [H,W,2]\n",
    "        L_np = gray_tensor[0,0].cpu().numpy() # [H,W]\n",
    "\n",
    "        # Lab->RGB 복원 (L은 gray에서 가져옴)\n",
    "        pred_rgb = lab_to_rgb(L_np, pred_ab_np[:,:,0], pred_ab_np[:,:,1])\n",
    "\n",
    "        # 결과 저장\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        Image.fromarray((pred_rgb*255).astype(np.uint8)).save(out_path)\n",
    "        print(f\"Saved restored image: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 손상복원모델\n",
    "\n",
    "![epoch14](https://github.com/Geon-05/submit_imagepro/blob/main/%EC%A0%9C%EC%B6%9C%EC%9A%A9%EC%9D%B4%EB%AF%B8%EC%A7%80/%EC%86%90%EC%83%81%EB%B3%B5%EC%9B%90.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "#---------------------------\n",
    "# 파라미터 설정\n",
    "#---------------------------\n",
    "batch_size = 4\n",
    "lr = 0.0002\n",
    "epochs = 50  # 추가 학습할 에폭 수\n",
    "test_size = 0.2\n",
    "num_workers = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dir = \"/root/.cache/kagglehub/datasets/geon05/damages-masks/versions/1/damage_images/damage_images\"\n",
    "gt_dir = \"/root/.cache/kagglehub/datasets/geon05/dataset2/versions/1/train_gt\"\n",
    "mask_dir = \"/root/.cache/kagglehub/datasets/geon05/damages-masks/versions/1/output_masks/output_masks\"\n",
    "\n",
    "#---------------------------\n",
    "# Perceptual Loss 정의\n",
    "#---------------------------\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer_ids=[0, 5, 10, 19, 28], requires_grad=False):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        self.layers = nn.ModuleList([vgg[i] for i in layer_ids])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = 0.0\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            loss += nn.functional.l1_loss(x, y)\n",
    "        return loss\n",
    "\n",
    "vgg_mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
    "vgg_std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
    "\n",
    "def normalize_vgg_inputs(img):\n",
    "    return (img - vgg_mean) / vgg_std\n",
    "\n",
    "#---------------------------\n",
    "# Gated Convolution Layer 정의\n",
    "#---------------------------\n",
    "class GatedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, activation=nn.ReLU()):\n",
    "        super(GatedConv2d, self).__init__()\n",
    "        self.feature_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.feature_conv(x)\n",
    "        m = self.mask_conv(x)\n",
    "        gated = self.sigmoid(m)\n",
    "        if self.activation is not None:\n",
    "            f = self.activation(f)\n",
    "        return f * gated\n",
    "\n",
    "class GatedDeconv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, activation=nn.ReLU()):\n",
    "        super(GatedDeconv2d, self).__init__()\n",
    "        self.feature_deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.mask_deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.feature_deconv(x)\n",
    "        m = self.mask_deconv(x)\n",
    "        gated = self.sigmoid(m)\n",
    "        if self.activation is not None:\n",
    "            f = self.activation(f)\n",
    "        return f * gated\n",
    "\n",
    "#---------------------------\n",
    "# Contextual Attention (간략 예시)\n",
    "#---------------------------\n",
    "class ContextualAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=3, stride=1, dilation=1):\n",
    "        super(ContextualAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 512, kernel_size, stride, dilation, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.size()\n",
    "        query = x.view(B,C,-1)\n",
    "        key = x.view(B,C,-1)\n",
    "        value = x.view(B,C,-1)\n",
    "\n",
    "        attn = torch.bmm(query.permute(0,2,1), key)\n",
    "        attn = self.softmax(attn)\n",
    "        out = torch.bmm(attn, value.permute(0,2,1))\n",
    "        out = out.permute(0,2,1).view(B,C,H,W)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class InpaintGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InpaintGenerator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            GatedConv2d(4, 64, 4, 2, 1),\n",
    "            GatedConv2d(64, 128, 4, 2, 1),\n",
    "            GatedConv2d(128, 256, 4, 2, 1),\n",
    "            GatedConv2d(256, 512, 4, 2, 1)\n",
    "        )\n",
    "        self.contextual_attention = ContextualAttention()\n",
    "        self.decoder = nn.Sequential(\n",
    "            GatedDeconv2d(512, 256, 4, 2, 1),\n",
    "            GatedDeconv2d(256, 128, 4, 2, 1),\n",
    "            GatedDeconv2d(128, 64, 4, 2, 1),\n",
    "            GatedDeconv2d(64, 64, 4, 2, 1, activation=nn.ReLU()),\n",
    "            nn.Conv2d(64, 3, 3, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        inp = torch.cat((x, mask), dim=1)\n",
    "        feat = self.encoder(inp)\n",
    "        feat = self.contextual_attention(feat)\n",
    "        out = self.decoder(feat)\n",
    "        return out\n",
    "\n",
    "class InpaintDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InpaintDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128,256,4,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256,1,4,1,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#---------------------------\n",
    "# Dataset 정의\n",
    "#---------------------------\n",
    "class InpaintDataset(Dataset):\n",
    "    def __init__(self, input_paths, gt_paths, mask_paths, transform=None):\n",
    "        self.input_paths = input_paths\n",
    "        self.gt_paths = gt_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = Image.open(self.input_paths[idx]).convert(\"RGB\")\n",
    "        gt = Image.open(self.gt_paths[idx]).convert(\"RGB\")\n",
    "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            inp = self.transform(inp)\n",
    "            gt = self.transform(gt)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return inp, gt, mask\n",
    "\n",
    "#---------------------------\n",
    "# 파일 경로 수집\n",
    "#---------------------------\n",
    "input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(('png','jpg','jpeg'))])\n",
    "gt_files = sorted([os.path.join(gt_dir, f) for f in os.listdir(gt_dir) if f.endswith(('png','jpg','jpeg'))])\n",
    "mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith(('png','jpg','jpeg'))])\n",
    "\n",
    "train_input_paths, val_input_paths, train_gt_paths, val_gt_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
    "    input_files, gt_files, mask_files, test_size=test_size, random_state=42)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((512,512)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = InpaintDataset(train_input_paths, train_gt_paths, train_mask_paths, transform=transform)\n",
    "val_dataset = InpaintDataset(val_input_paths, val_gt_paths, val_mask_paths, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#---------------------------\n",
    "# 모델 초기화 및 이전 모델 가중치 로드\n",
    "#---------------------------\n",
    "generator = InpaintGenerator().to(device)\n",
    "discriminator = InpaintDiscriminator().to(device)\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "l1_loss = nn.L1Loss()\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "perceptual_criterion = VGGPerceptualLoss().to(device)\n",
    "\n",
    "pretrained_generator_path = \"/content/2024_03/deepfillv2_generator_finetune_epoch11.pth\"\n",
    "pretrained_discriminator_path = \"/content/2024_03/deepfillv2_discriminator_finetune_epoch11.pth\"\n",
    "\n",
    "generator.load_state_dict(torch.load(pretrained_generator_path, map_location=device, weights_only=True))\n",
    "discriminator.load_state_dict(torch.load(pretrained_discriminator_path, map_location=device, weights_only=True))\n",
    "\n",
    "print(\"이전 학습 가중치를 로드하였습니다. 동일 데이터셋에서 이어서 학습을 진행합니다.\")\n",
    "\n",
    "save_dir = \"2024_03\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 최적 모델 저장을 위한 변수\n",
    "best_val_loss = float('inf')  # 현재까지의 최소 validation loss 기록\n",
    "\n",
    "#---------------------------\n",
    "# 추가 학습 루프\n",
    "#---------------------------\n",
    "for epoch in range(epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    running_g_loss = 0.0\n",
    "    running_d_loss = 0.0\n",
    "\n",
    "    for i, (inp, gt, mask) in enumerate(tqdm(train_dataloader, desc=f\"Additional Epoch [{epoch+1}/{epochs}]\")):\n",
    "        inp, gt, mask = inp.to(device), gt.to(device), mask.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        # Train Discriminator\n",
    "        # -----------------\n",
    "        fake = generator(inp, mask)\n",
    "        real_pred = discriminator(gt)\n",
    "        fake_pred = discriminator(fake.detach())\n",
    "\n",
    "        real_label = torch.ones_like(real_pred).to(device)\n",
    "        fake_label = torch.zeros_like(fake_pred).to(device)\n",
    "\n",
    "        d_loss_real = bce_loss(real_pred, real_label)\n",
    "        d_loss_fake = bce_loss(fake_pred, fake_label)\n",
    "        d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # -----------------\n",
    "        # Train Generator\n",
    "        # -----------------\n",
    "        fake_pred = discriminator(fake)\n",
    "        real_label_g = torch.ones_like(fake_pred).to(device)\n",
    "        g_adv_loss = bce_loss(fake_pred, real_label_g)\n",
    "\n",
    "        fake_norm = normalize_vgg_inputs(fake)\n",
    "        gt_norm = normalize_vgg_inputs(gt)\n",
    "\n",
    "        g_l1 = l1_loss(fake, gt)\n",
    "        g_mse = mse_loss(fake, gt)\n",
    "        g_perc = perceptual_criterion(fake_norm, gt_norm)\n",
    "\n",
    "        g_recon_loss = 10.0 * g_l1 + 5.0 * g_mse + 1.0 * g_perc\n",
    "        g_loss = g_adv_loss + g_recon_loss\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        running_g_loss += g_loss.item()\n",
    "        running_d_loss += d_loss.item()\n",
    "\n",
    "    avg_g_loss = running_g_loss / len(train_dataloader)\n",
    "    avg_d_loss = running_d_loss / len(train_dataloader)\n",
    "\n",
    "    print(f\"Additional Epoch [{epoch+1}/{epochs}] - G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    generator.eval()\n",
    "    val_l1 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp, gt, mask in val_dataloader:\n",
    "            inp, gt, mask = inp.to(device), gt.to(device), mask.to(device)\n",
    "            fake = generator(inp, mask)\n",
    "            val_l1 += l1_loss(fake, gt).item()\n",
    "\n",
    "    val_l1 = val_l1 / len(val_dataloader)\n",
    "    print(f\"Validation L1 Loss: {val_l1:.4f}\")\n",
    "\n",
    "    # 매 epoch 종료 시 모델 저장\n",
    "    torch.save(generator.state_dict(), f\"{save_dir}/01_deepfillv2_generator_finetune_epoch{epoch+1}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"{save_dir}/01_deepfillv2_discriminator_finetune_epoch{epoch+1}.pth\")\n",
    "\n",
    "    # 최적 모델 갱신\n",
    "    if val_l1 < best_val_loss:\n",
    "        best_val_loss = val_l1\n",
    "        torch.save(generator.state_dict(), f\"{save_dir}/best_generator.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"{save_dir}/best_discriminator.pth\")\n",
    "        print(f\"최적 모델 갱신: Validation L1 Loss: {val_l1:.4f}, 모델 저장 완료.\")\n",
    "\n",
    "    # 시각화(예시)\n",
    "    with torch.no_grad():\n",
    "        sample_inp, sample_gt, sample_mask = next(iter(val_dataloader))\n",
    "        sample_inp, sample_gt, sample_mask = sample_inp.to(device), sample_gt.to(device), sample_mask.to(device)\n",
    "        sample_fake = generator(sample_inp, sample_mask)\n",
    "\n",
    "        fig, axes = plt.subplots(3,3, figsize=(12,12))\n",
    "        for idx in range(3):\n",
    "            axes[idx,0].imshow(sample_inp[idx].cpu().permute(1,2,0).numpy())\n",
    "            axes[idx,0].set_title(\"Corrupted\")\n",
    "            axes[idx,0].axis(\"off\")\n",
    "\n",
    "            axes[idx,1].imshow(sample_gt[idx].cpu().permute(1,2,0).numpy())\n",
    "            axes[idx,1].set_title(\"Ground Truth\")\n",
    "            axes[idx,1].axis(\"off\")\n",
    "\n",
    "            axes[idx,2].imshow(sample_fake[idx].cpu().permute(1,2,0).numpy())\n",
    "            axes[idx,2].set_title(\"Inpainted\")\n",
    "            axes[idx,2].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############################\n",
    "# 모델 구조 정의 (학습시 사용한 것과 동일)\n",
    "############################\n",
    "\n",
    "class GatedConv2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, activation=torch.nn.ReLU()):\n",
    "        super(GatedConv2d, self).__init__()\n",
    "        self.feature_conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.mask_conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.feature_conv(x)\n",
    "        m = self.mask_conv(x)\n",
    "        gated = self.sigmoid(m)\n",
    "        if self.activation is not None:\n",
    "            f = self.activation(f)\n",
    "        return f * gated\n",
    "\n",
    "class GatedDeconv2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, activation=torch.nn.ReLU()):\n",
    "        super(GatedDeconv2d, self).__init__()\n",
    "        self.feature_deconv = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.mask_deconv = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.feature_deconv(x)\n",
    "        m = self.mask_deconv(x)\n",
    "        gated = self.sigmoid(m)\n",
    "        if self.activation is not None:\n",
    "            f = self.activation(f)\n",
    "        return f * gated\n",
    "\n",
    "class ContextualAttention(torch.nn.Module):\n",
    "    def __init__(self, kernel_size=3, stride=1, dilation=1):\n",
    "        super(ContextualAttention, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(512, 512, kernel_size, stride, dilation, bias=False)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.size()\n",
    "        query = x.view(B,C,-1)\n",
    "        key = x.view(B,C,-1)\n",
    "        value = x.view(B,C,-1)\n",
    "        attn = torch.bmm(query.permute(0,2,1), key)\n",
    "        attn = self.softmax(attn)\n",
    "        out = torch.bmm(attn, value.permute(0,2,1))\n",
    "        out = out.permute(0,2,1).view(B,C,H,W)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class Stage1Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1Generator, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            GatedConv2d(4, 64, 4, 2, 1),\n",
    "            GatedConv2d(64, 128, 4, 2, 1),\n",
    "            GatedConv2d(128, 256, 4, 2, 1),\n",
    "            GatedConv2d(256, 512, 4, 2, 1)\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            GatedDeconv2d(512, 256, 4, 2, 1),\n",
    "            GatedDeconv2d(256, 128, 4, 2, 1),\n",
    "            GatedDeconv2d(128, 64, 4, 2, 1),\n",
    "            GatedDeconv2d(64, 64, 4, 2, 1, activation=torch.nn.ReLU()),\n",
    "            torch.nn.Conv2d(64, 3, 3, 1, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        inp = torch.cat((x, mask), dim=1)\n",
    "        feat = self.encoder(inp)\n",
    "        out = self.decoder(feat)\n",
    "        return out\n",
    "\n",
    "class Stage2Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage2Generator, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            GatedConv2d(7, 64, 4, 2, 1),\n",
    "            GatedConv2d(64, 128, 4, 2, 1),\n",
    "            GatedConv2d(128, 256, 4, 2, 1),\n",
    "            GatedConv2d(256, 512, 4, 2, 1)\n",
    "        )\n",
    "        self.contextual_attention = ContextualAttention()\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            GatedDeconv2d(512, 256, 4, 2, 1),\n",
    "            GatedDeconv2d(256, 128, 4, 2, 1),\n",
    "            GatedDeconv2d(128, 64, 4, 2, 1),\n",
    "            GatedDeconv2d(64, 64, 4, 2, 1, activation=torch.nn.ReLU()),\n",
    "            torch.nn.Conv2d(64, 3, 3, 1, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, coarse_out, inp, mask):\n",
    "        fin_inp = torch.cat((coarse_out, inp, mask), dim=1)\n",
    "        feat = self.encoder(fin_inp)\n",
    "        feat = self.contextual_attention(feat)\n",
    "        out = self.decoder(feat)\n",
    "        return out\n",
    "\n",
    "# 최적 모델 가중치 경로\n",
    "best_coarse_path = \"/home/zqrc05/project/imagepro/test/model/colToperPlus/12_best_coarse_generator_epoch9.pth\"\n",
    "best_fine_path = \"/home/zqrc05/project/imagepro/test/model/colToperPlus/12_best_fine_generator_epoch9.pth\"\n",
    "\n",
    "\n",
    "# 테스트 이미지(손상된 컬러 이미지) 폴더\n",
    "test_input_dir = \"/home/zqrc05/project/imagepro/test/output_grayTocol\"\n",
    "test_mask_dir = \"/home/zqrc05/project/imagepro/test/mask\"\n",
    "output_dir = \"/home/zqrc05/project/imagepro/test/output_colToper_13\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "coarse_generator = Stage1Generator().to(device)\n",
    "fine_generator = Stage2Generator().to(device)\n",
    "\n",
    "coarse_generator.load_state_dict(torch.load(best_coarse_path, map_location=device, weights_only=True))\n",
    "fine_generator.load_state_dict(torch.load(best_fine_path, map_location=device, weights_only=True))\n",
    "\n",
    "coarse_generator.eval()\n",
    "fine_generator.eval()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((512,512)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "test_files = [f for f in os.listdir(test_input_dir) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for filename in test_files:\n",
    "        input_path = os.path.join(test_input_dir, filename)\n",
    "        mask_path = os.path.join(test_mask_dir, filename)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"{mask_path}가 존재하지 않습니다. 스킵합니다.\")\n",
    "            continue\n",
    "\n",
    "        inp_img = Image.open(input_path).convert(\"RGB\")\n",
    "        mask_img = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        inp_tensor = transform(inp_img).unsqueeze(0).to(device)   # (1,3,H,W)\n",
    "        mask_tensor = transform(mask_img).unsqueeze(0).to(device) # (1,1,H,W)\n",
    "\n",
    "        # 손상 영역 0 처리\n",
    "        mask_broadcast = mask_tensor.expand_as(inp_tensor)\n",
    "        damaged_inp = inp_tensor * (1 - mask_broadcast)\n",
    "\n",
    "        # 복원\n",
    "        coarse_out = coarse_generator(damaged_inp, mask_tensor)\n",
    "        fine_out = fine_generator(coarse_out, damaged_inp, mask_tensor)\n",
    "\n",
    "        # 여기서 복원된 부분(fine_out)을 마스크가 1인 영역에만 적용\n",
    "        # final_result = original_damaged_image * (1 - mask) + fine_out * mask\n",
    "        final_result = inp_tensor * (1 - mask_broadcast) + fine_out * mask_broadcast\n",
    "\n",
    "        # 결과 텐서를 이미지로 변환\n",
    "        final_result_pil = T.ToPILImage()(final_result.squeeze(0).cpu())\n",
    "\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        final_result_pil.save(save_path)\n",
    "        print(f\"{filename} 복원 완료(마스크 영역만 덮어쓰기) -> {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 마스크 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 주어진 mask_function (유저 코드)\n",
    "def mask_function(input_path, gt_path):\n",
    "    try:\n",
    "        # Load and preprocess images\n",
    "        input_image = Image.open(input_path).convert(\"RGB\")  # Ensure RGB format\n",
    "        input_image_np = np.array(input_image)\n",
    "        gt_image_gray = Image.open(gt_path).convert(\"L\")  # Load mask as grayscale\n",
    "        gt_image_gray_np = np.array(gt_image_gray)\n",
    "\n",
    "        # Convert input_image_np to grayscale\n",
    "        input_image_gray_np = cv2.cvtColor(input_image_np, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Compute the difference\n",
    "        difference = cv2.absdiff(gt_image_gray_np, input_image_gray_np)\n",
    "\n",
    "        # Threshold the difference to create a binary mask\n",
    "        _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Remove small noise with morphological operations\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        binary_difference = cv2.morphologyEx(binary_difference, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary_difference, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Fill the contours to create a mask\n",
    "        mask_filled = np.zeros_like(binary_difference)\n",
    "        cv2.drawContours(mask_filled, contours, -1, color=255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Expand the filled mask (dilation)\n",
    "        mask_filled = cv2.dilate(mask_filled, kernel, iterations=1)\n",
    "\n",
    "        # Convert input image and mask to PyTorch tensors\n",
    "        mask_tensor = torch.tensor(mask_filled, dtype=torch.float32).unsqueeze(0) / 255.0  # Normalize mask to [0, 1]\n",
    "\n",
    "        return mask_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"Mask creation failed for input: {input_path}, error: {e}\")\n",
    "        # 기본적으로 0으로 된 마스크를 반환 (모든 값이 0인 빈 마스크)\n",
    "        return torch.zeros((1, 512, 512), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 손상 이미지 폴더, 정답 이미지 폴더, 출력 마스크 폴더 설정\n",
    "input_images_dir = \"train_input\"  # 손상 이미지 폴더 경로\n",
    "gt_images_dir = \"train_gt\"        # 정답 이미지 폴더 경로\n",
    "output_masks_dir = \"output_masks\"  # 결과 마스크를 저장할 폴더 경로\n",
    "\n",
    "os.makedirs(output_masks_dir, exist_ok=True)  # 출력 폴더가 없으면 생성\n",
    "\n",
    "# 손상 이미지 폴더에 있는 모든 이미지 파일 이름 리스트 획득\n",
    "input_image_files = sorted([f for f in os.listdir(input_images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "for filename in input_image_files:\n",
    "    input_path = os.path.join(input_images_dir, filename)\n",
    "    gt_path = os.path.join(gt_images_dir, filename)\n",
    "\n",
    "    # 마스크 생성\n",
    "    mask_tensor = mask_function(input_path, gt_path)\n",
    "\n",
    "    # mask_tensor는 shape [1, H, W]의 텐서이며 값 범위 [0,1]\n",
    "    # 이를 이미지로 저장하기 위해 numpy로 변환 후 0~255 범위로 스케일링\n",
    "    mask_np = (mask_tensor.squeeze(0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # 마스크 이미지 저장\n",
    "    output_path = os.path.join(output_masks_dir, filename)\n",
    "    mask_img = Image.fromarray(mask_np)\n",
    "    mask_img.save(output_path)\n",
    "\n",
    "    print(f\"Saved mask for {filename} at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 컬러 손상 이미지 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def mask_function(input_path, gt_path):\n",
    "    try:\n",
    "        # Load and preprocess images\n",
    "        input_image = Image.open(input_path).convert(\"RGB\")  # Ensure RGB format\n",
    "        input_image_np = np.array(input_image)\n",
    "        gt_image_gray = Image.open(gt_path).convert(\"L\")  # Load mask as grayscale\n",
    "        gt_image_gray_np = np.array(gt_image_gray)\n",
    "\n",
    "        # Convert input_image_np to grayscale\n",
    "        input_image_gray_np = cv2.cvtColor(input_image_np, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Compute the difference\n",
    "        difference = cv2.absdiff(gt_image_gray_np, input_image_gray_np)\n",
    "\n",
    "        # Threshold the difference to create a binary mask\n",
    "        _, binary_difference = cv2.threshold(difference, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Remove small noise with morphological operations\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        binary_difference = cv2.morphologyEx(binary_difference, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary_difference, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Fill the contours to create a mask\n",
    "        mask_filled = np.zeros_like(binary_difference)\n",
    "        cv2.drawContours(mask_filled, contours, -1, color=255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Expand the filled mask (dilation)\n",
    "        mask_filled = cv2.dilate(mask_filled, kernel, iterations=1)\n",
    "\n",
    "        return mask_filled  # Return mask as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Mask creation failed for input: {input_path}, error: {e}\")\n",
    "        return np.zeros((512, 512), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def apply_existing_mask(image, mask):\n",
    "    damaged_image = image.copy()\n",
    "    damaged_image[mask == 255] = 0\n",
    "    return damaged_image\n",
    "\n",
    "\n",
    "def process_images_in_folder(input_folder, output_folder, damage_folder):\n",
    "    # 출력 폴더가 없으면 생성\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 입력 폴더의 이미지 처리\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            damage_path = os.path.join(damage_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # 이미지 로드\n",
    "            image = cv2.imread(input_path)\n",
    "            if image is None:\n",
    "                print(f\"Skipping {filename}: unable to read file.\")\n",
    "                continue\n",
    "\n",
    "            # 마스크 생성\n",
    "            mask = mask_function(damage_path, input_path)\n",
    "\n",
    "            # 마스크 적용\n",
    "            damaged_image = apply_existing_mask(image, mask)\n",
    "\n",
    "            # 결과 저장 (손상된 파일명 동일하게 저장)\n",
    "            cv2.imwrite(output_path, damaged_image)\n",
    "            print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "\n",
    "# 사용 예제\n",
    "input_folder = \"train_gt\"  # 원본 이미지 폴더 경로\n",
    "damage_folder = \"train_input\"  # 손상 기준 폴더 경로\n",
    "output_folder = \"damage_images\"  # 손상된 이미지 저장 폴더 경로\n",
    "\n",
    "process_images_in_folder(input_folder, output_folder, damage_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagepro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
